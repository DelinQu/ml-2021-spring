# Deep Learning
- Youtube:
  - General Guidance: [Chinese](https://www.youtube.com/watch?v=WeHM2xpYQpw) [English](https://youtu.be/3qgKpBptyFY)
  - When Gradient Is Small: Local Minimum and Saddle Point: [Chinese](https://www.youtube.com/watch?v=QW6uINn7uGk) [English](https://youtu.be/yz7QS1I6omw)
  - Tips for Training: Batch and Momentum: [Chinese](https://www.youtube.com/watch?v=zzbr1h9sF54) [English](https://youtu.be/MNoEQ9w-AbE)
  - Tips for Training: Adaptive Learning Rate: [Chinese](https://www.youtube.com/watch?v=HYUXEeh3kwY) [English](https://www.youtube.com/watch?v=8yf-tU7zm7w)
  - Loss Function: Classification: [Chinese](https://www.youtube.com/watch?v=O2VkP8dJ5FE) [English](https://www.youtube.com/watch?v=jqVONJ-Wn8w)
- Slides:
  - Guideline of ML: overfit: [ppt](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/overfit-v6.pptx) [pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/overfit-v6.pdf)
  - Critical Point: small gradient: [ppt](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/small-gradient-v7.pptx) [pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/small-gradient-v7.pdf)
  - Adaptive Learning Rate: optimizer: [ppt](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/optimizer_v4.pptx) [pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/optimizer_v4.pdf)
  - Loss of Classification: classification: [ppt](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/classification_v2.pptx) [pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/classification_v2.pdf)
- HW2: Classification

---
## General Guidance
- Framework of ML
- Model Bias 
- Optimization Issue
- Overfitting
- Cross Validation
- N-fold Cross Validation
- Mismatch

---
## Critical Point: small gradient
- critical points
- Tayler Series Approximation
- Hessian
- Saddle Point v.s. Local Minima
- Empirical Study

---
## Tips for training: Batch and Momentum
- Small Batch v.s. Large Batch
- (Vanilla) Gradient Descent
- Gradient Descent + Momentum

---
## Adaptive Learning Rate: optimizer
- Training stuck â‰  Small Gradient
- Training can be difficult even without critical points
- Root Mean Square
- RMSProp
- Adam: RMSProp + Momentum
- Learning Rate Scheduling
- Summary of Optimization

---
## Classification
- Class as one-hot vector
- Soft-max
- Cross-entropy
- Soft-max is binded with Cross-entropy in Pytorch
